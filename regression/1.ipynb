{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A linear regression learning algorithm example \n",
    "using TensorFlow library.\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import housing data data\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions to read input boston dataset.\n",
    "def read_dataset(filePath,delimiter=','):\n",
    "    return genfromtxt(filePath, delimiter=delimiter)\n",
    "def read_boston_data():\n",
    "    data = load_boston()\n",
    "    X = np.array(data.data)\n",
    "    mu = np.mean(X,axis=0)\n",
    "    sigma = np.std(X,axis=0)\n",
    "    return (X - mu)/sigma, np.array(data.target)\n",
    "def append_bias_reshape(features,labels):\n",
    "    n_training_samples = features.shape[0]\n",
    "    n_dim = features.shape[1]\n",
    "    f = np.reshape(np.c_[np.ones(n_training_samples),features],[n_training_samples,n_dim + 1])\n",
    "    l = np.reshape(labels,[n_training_samples,1])\n",
    "    return f, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features,labels = read_boston_data()\n",
    "f, l = append_bias_reshape(features,labels)\n",
    "n_dim = f.shape[1]\n",
    "train_x, test_x, train_y, test_y = train_test_split(f, l, test_size=0.80, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 200\n",
    "logs_path=\"/tmp/logs/3/1\"\n",
    "\n",
    "# tf Graph Input\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32,[None,n_dim])\n",
    "    y = tf.placeholder(tf.float32,[None,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Weights:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    W = tf.Variable(tf.zeros([n_dim, 1]))\n",
    "    b = tf.Variable(tf.zeros([1]))\n",
    "    tf.summary.histogram(\"Weights\",W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "with tf.name_scope('model'):\n",
    "    layer1 = tf.matmul(x, W) + b\n",
    "    pred = layer1\n",
    "    # Minimize error using mean-square error\n",
    "    cost = tf.reduce_mean(tf.square(layer1 - y))\n",
    "    tf.summary.scalar('training_cost',cost)    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 584.592529297\n",
      "Epoch: 0002 cost= 536.159973145\n",
      "Epoch: 0003 cost= 492.447906494\n",
      "Epoch: 0004 cost= 452.898284912\n",
      "Epoch: 0005 cost= 417.034790039\n",
      "Epoch: 0006 cost= 384.448760986\n",
      "Epoch: 0007 cost= 354.787841797\n",
      "Epoch: 0008 cost= 327.746429443\n",
      "Epoch: 0009 cost= 303.058502197\n",
      "Epoch: 0010 cost= 280.491058350\n",
      "Epoch: 0011 cost= 259.839172363\n",
      "Epoch: 0012 cost= 240.921859741\n",
      "Epoch: 0013 cost= 223.578460693\n",
      "Epoch: 0014 cost= 207.665969849\n",
      "Epoch: 0015 cost= 193.056564331\n",
      "Epoch: 0016 cost= 179.635574341\n",
      "Epoch: 0017 cost= 167.299896240\n",
      "Epoch: 0018 cost= 155.956527710\n",
      "Epoch: 0019 cost= 145.521301270\n",
      "Epoch: 0020 cost= 135.918060303\n",
      "Epoch: 0021 cost= 127.077575684\n",
      "Epoch: 0022 cost= 118.936874390\n",
      "Epoch: 0023 cost= 111.438591003\n",
      "Epoch: 0024 cost= 104.530364990\n",
      "Epoch: 0025 cost= 98.164413452\n",
      "Epoch: 0026 cost= 92.296951294\n",
      "Epoch: 0027 cost= 86.887969971\n",
      "Epoch: 0028 cost= 81.900772095\n",
      "Epoch: 0029 cost= 77.301750183\n",
      "Epoch: 0030 cost= 73.059989929\n",
      "Epoch: 0031 cost= 69.147193909\n",
      "Epoch: 0032 cost= 65.537353516\n",
      "Epoch: 0033 cost= 62.206520081\n",
      "Epoch: 0034 cost= 59.132724762\n",
      "Epoch: 0035 cost= 56.295749664\n",
      "Epoch: 0036 cost= 53.677009583\n",
      "Epoch: 0037 cost= 51.259384155\n",
      "Epoch: 0038 cost= 49.027141571\n",
      "Epoch: 0039 cost= 46.965778351\n",
      "Epoch: 0040 cost= 45.061939240\n",
      "Epoch: 0041 cost= 43.303363800\n",
      "Epoch: 0042 cost= 41.678703308\n",
      "Epoch: 0043 cost= 40.177539825\n",
      "Epoch: 0044 cost= 38.790287018\n",
      "Epoch: 0045 cost= 37.508071899\n",
      "Epoch: 0046 cost= 36.322769165\n",
      "Epoch: 0047 cost= 35.226840973\n",
      "Epoch: 0048 cost= 34.213382721\n",
      "Epoch: 0049 cost= 33.276008606\n",
      "Epoch: 0050 cost= 32.408843994\n",
      "Epoch: 0051 cost= 31.606454849\n",
      "Epoch: 0052 cost= 30.863857269\n",
      "Epoch: 0053 cost= 30.176441193\n",
      "Epoch: 0054 cost= 29.539968491\n",
      "Epoch: 0055 cost= 28.950511932\n",
      "Epoch: 0056 cost= 28.404470444\n",
      "Epoch: 0057 cost= 27.898496628\n",
      "Epoch: 0058 cost= 27.429544449\n",
      "Epoch: 0059 cost= 26.994770050\n",
      "Epoch: 0060 cost= 26.591564178\n",
      "Epoch: 0061 cost= 26.217521667\n",
      "Epoch: 0062 cost= 25.870414734\n",
      "Epoch: 0063 cost= 25.548194885\n",
      "Epoch: 0064 cost= 25.248971939\n",
      "Epoch: 0065 cost= 24.970998764\n",
      "Epoch: 0066 cost= 24.712675095\n",
      "Epoch: 0067 cost= 24.472511292\n",
      "Epoch: 0068 cost= 24.249135971\n",
      "Epoch: 0069 cost= 24.041282654\n",
      "Epoch: 0070 cost= 23.847793579\n",
      "Epoch: 0071 cost= 23.667581558\n",
      "Epoch: 0072 cost= 23.499658585\n",
      "Epoch: 0073 cost= 23.343099594\n",
      "Epoch: 0074 cost= 23.197065353\n",
      "Epoch: 0075 cost= 23.060768127\n",
      "Epoch: 0076 cost= 22.933492661\n",
      "Epoch: 0077 cost= 22.814559937\n",
      "Epoch: 0078 cost= 22.703363419\n",
      "Epoch: 0079 cost= 22.599327087\n",
      "Epoch: 0080 cost= 22.501934052\n",
      "Epoch: 0081 cost= 22.410692215\n",
      "Epoch: 0082 cost= 22.325153351\n",
      "Epoch: 0083 cost= 22.244903564\n",
      "Epoch: 0084 cost= 22.169559479\n",
      "Epoch: 0085 cost= 22.098768234\n",
      "Epoch: 0086 cost= 22.032199860\n",
      "Epoch: 0087 cost= 21.969552994\n",
      "Epoch: 0088 cost= 21.910549164\n",
      "Epoch: 0089 cost= 21.854925156\n",
      "Epoch: 0090 cost= 21.802450180\n",
      "Epoch: 0091 cost= 21.752889633\n",
      "Epoch: 0092 cost= 21.706048965\n",
      "Epoch: 0093 cost= 21.661737442\n",
      "Epoch: 0094 cost= 21.619773865\n",
      "Epoch: 0095 cost= 21.579999924\n",
      "Epoch: 0096 cost= 21.542264938\n",
      "Epoch: 0097 cost= 21.506427765\n",
      "Epoch: 0098 cost= 21.472362518\n",
      "Epoch: 0099 cost= 21.439943314\n",
      "Epoch: 0100 cost= 21.409067154\n",
      "Epoch: 0101 cost= 21.379631042\n",
      "Epoch: 0102 cost= 21.351530075\n",
      "Epoch: 0103 cost= 21.324686050\n",
      "Epoch: 0104 cost= 21.299011230\n",
      "Epoch: 0105 cost= 21.274436951\n",
      "Epoch: 0106 cost= 21.250879288\n",
      "Epoch: 0107 cost= 21.228284836\n",
      "Epoch: 0108 cost= 21.206592560\n",
      "Epoch: 0109 cost= 21.185745239\n",
      "Epoch: 0110 cost= 21.165679932\n",
      "Epoch: 0111 cost= 21.146362305\n",
      "Epoch: 0112 cost= 21.127738953\n",
      "Epoch: 0113 cost= 21.109771729\n",
      "Epoch: 0114 cost= 21.092416763\n",
      "Epoch: 0115 cost= 21.075643539\n",
      "Epoch: 0116 cost= 21.059412003\n",
      "Epoch: 0117 cost= 21.043699265\n",
      "Epoch: 0118 cost= 21.028465271\n",
      "Epoch: 0119 cost= 21.013690948\n",
      "Epoch: 0120 cost= 20.999351501\n",
      "Epoch: 0121 cost= 20.985414505\n",
      "Epoch: 0122 cost= 20.971862793\n",
      "Epoch: 0123 cost= 20.958679199\n",
      "Epoch: 0124 cost= 20.945837021\n",
      "Epoch: 0125 cost= 20.933324814\n",
      "Epoch: 0126 cost= 20.921119690\n",
      "Epoch: 0127 cost= 20.909214020\n",
      "Epoch: 0128 cost= 20.897590637\n",
      "Epoch: 0129 cost= 20.886226654\n",
      "Epoch: 0130 cost= 20.875118256\n",
      "Epoch: 0131 cost= 20.864259720\n",
      "Epoch: 0132 cost= 20.853620529\n",
      "Epoch: 0133 cost= 20.843202591\n",
      "Epoch: 0134 cost= 20.832994461\n",
      "Epoch: 0135 cost= 20.822988510\n",
      "Epoch: 0136 cost= 20.813175201\n",
      "Epoch: 0137 cost= 20.803544998\n",
      "Epoch: 0138 cost= 20.794084549\n",
      "Epoch: 0139 cost= 20.784797668\n",
      "Epoch: 0140 cost= 20.775669098\n",
      "Epoch: 0141 cost= 20.766696930\n",
      "Epoch: 0142 cost= 20.757873535\n",
      "Epoch: 0143 cost= 20.749193192\n",
      "Epoch: 0144 cost= 20.740652084\n",
      "Epoch: 0145 cost= 20.732238770\n",
      "Epoch: 0146 cost= 20.723964691\n",
      "Epoch: 0147 cost= 20.715799332\n",
      "Epoch: 0148 cost= 20.707763672\n",
      "Epoch: 0149 cost= 20.699840546\n",
      "Epoch: 0150 cost= 20.692024231\n",
      "Epoch: 0151 cost= 20.684318542\n",
      "Epoch: 0152 cost= 20.676715851\n",
      "Epoch: 0153 cost= 20.669216156\n",
      "Epoch: 0154 cost= 20.661813736\n",
      "Epoch: 0155 cost= 20.654504776\n",
      "Epoch: 0156 cost= 20.647283554\n",
      "Epoch: 0157 cost= 20.640161514\n",
      "Epoch: 0158 cost= 20.633115768\n",
      "Epoch: 0159 cost= 20.626163483\n",
      "Epoch: 0160 cost= 20.619287491\n",
      "Epoch: 0161 cost= 20.612491608\n",
      "Epoch: 0162 cost= 20.605775833\n",
      "Epoch: 0163 cost= 20.599134445\n",
      "Epoch: 0164 cost= 20.592565536\n",
      "Epoch: 0165 cost= 20.586067200\n",
      "Epoch: 0166 cost= 20.579643250\n",
      "Epoch: 0167 cost= 20.573287964\n",
      "Epoch: 0168 cost= 20.566997528\n",
      "Epoch: 0169 cost= 20.560771942\n",
      "Epoch: 0170 cost= 20.554609299\n",
      "Epoch: 0171 cost= 20.548511505\n",
      "Epoch: 0172 cost= 20.542472839\n",
      "Epoch: 0173 cost= 20.536497116\n",
      "Epoch: 0174 cost= 20.530576706\n",
      "Epoch: 0175 cost= 20.524715424\n",
      "Epoch: 0176 cost= 20.518913269\n",
      "Epoch: 0177 cost= 20.513162613\n",
      "Epoch: 0178 cost= 20.507469177\n",
      "Epoch: 0179 cost= 20.501827240\n",
      "Epoch: 0180 cost= 20.496234894\n",
      "Epoch: 0181 cost= 20.490695953\n",
      "Epoch: 0182 cost= 20.485206604\n",
      "Epoch: 0183 cost= 20.479766846\n",
      "Epoch: 0184 cost= 20.474372864\n",
      "Epoch: 0185 cost= 20.469030380\n",
      "Epoch: 0186 cost= 20.463733673\n",
      "Epoch: 0187 cost= 20.458482742\n",
      "Epoch: 0188 cost= 20.453273773\n",
      "Epoch: 0189 cost= 20.448114395\n",
      "Epoch: 0190 cost= 20.443000793\n",
      "Epoch: 0191 cost= 20.437925339\n",
      "Epoch: 0192 cost= 20.432895660\n",
      "Epoch: 0193 cost= 20.427904129\n",
      "Epoch: 0194 cost= 20.422954559\n",
      "Epoch: 0195 cost= 20.418046951\n",
      "Epoch: 0196 cost= 20.413179398\n",
      "Epoch: 0197 cost= 20.408348083\n",
      "Epoch: 0198 cost= 20.403558731\n",
      "Epoch: 0199 cost= 20.398807526\n",
      "Epoch: 0200 cost= 20.394096375\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "summary_op = tf.summary.merge_all()\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # create log writer object\n",
    "    writer = tf.summary.FileWriter(logs_path,sess.graph)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, c,summary = sess.run([optimizer, cost,summary_op], feed_dict={x: train_x,\n",
    "                                                          y: train_y})\n",
    "        # Compute average loss\n",
    "        avg_cost += c \n",
    "        writer.add_summary(summary)\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "    writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_auc = sess.run(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
