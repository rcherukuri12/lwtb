{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A logistic regression learning algorithm example using TensorFlow library.\n",
    "Note:- Logistic regression( this word is historical accident..it is really classification)\n",
    "This program also teaches how to add :\n",
    "1. Training metrics\n",
    "    a) Weights\n",
    "2. Test metrics \n",
    "    a) ROC and \n",
    "    b) Probability distribution to tensorboard.\n",
    "    \n",
    "3. Combined Loss chart for 'Training' and 'Testing'\n",
    "\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import shutil\n",
    "\n",
    "# Import cancer data\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions to read input \n",
    "def read_cancer_data():\n",
    "    cancer_data = load_breast_cancer()\n",
    "    features = np.array(cancer_data.data)\n",
    "    labels = np.array(cancer_data.target)\n",
    "    return features, labels\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis=0)\n",
    "    sigma = np.std(dataset,axis=0)\n",
    "    return (dataset - mu)/sigma\n",
    "def append_bias_reshape(features,labels):\n",
    "    n_training_samples = features.shape[0]\n",
    "    n_dim = features.shape[1]\n",
    "    f = np.reshape(np.c_[np.ones(n_training_samples),features],[n_training_samples,n_dim + 1])\n",
    "    l = np.reshape(labels,[n_training_samples,1])\n",
    "    return f, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features,labels = read_cancer_data()\n",
    "normalized_features = feature_normalize(features)\n",
    "f, l = append_bias_reshape(normalized_features,labels)\n",
    "n_dim = f.shape[1]\n",
    "train_x, test_x, train_y, test_y = train_test_split(f, l, test_size=0.80, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1000\n",
    "logs_path=\"/tmp/logs/3\"\n",
    "#############################\n",
    "# IMPORTANT:\n",
    "# Reset by deleting old logs before you start a run.\n",
    "####################################################\n",
    "shutil.rmtree(logs_path)\n",
    "\n",
    "# tf Graph Input\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32,[None,n_dim])\n",
    "    y = tf.placeholder(tf.float32,[None,1])\n",
    "    W = tf.Variable(tf.truncated_normal([n_dim, 1]))\n",
    "    train_wts = tf.summary.histogram(\"Weights\",W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "with tf.name_scope('model'):\n",
    "    layer1 = tf.matmul(x, W)\n",
    "    pred =  tf.nn.sigmoid(layer1) # sigmoid activation\n",
    "    # Minimize error using logistic regression\n",
    "    cost = tf.reduce_mean(tf.reduce_sum((-y * tf.log(pred)) - ((1 - y) * tf.log(1 - pred)), reduction_indices=[1]))\n",
    "    loss = tf.summary.scalar('loss',cost)\n",
    "    training_ops = tf.summary.merge([train_wts])\n",
    "    common_op = tf.summary.merge([loss])\n",
    "    #summary_op = tf.summary.merge_all()    \n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost= 1.714259267\n",
      "Epoch: 0100 cost= 0.965040028\n",
      "Epoch: 0150 cost= 0.665290356\n",
      "Epoch: 0200 cost= 0.502905011\n",
      "Epoch: 0250 cost= 0.401427805\n",
      "Epoch: 0300 cost= 0.336035848\n",
      "Epoch: 0350 cost= 0.293274581\n",
      "Epoch: 0400 cost= 0.264227569\n",
      "Epoch: 0450 cost= 0.242905140\n",
      "Epoch: 0500 cost= 0.226033300\n",
      "Epoch: 0550 cost= 0.211985722\n",
      "Epoch: 0600 cost= 0.199925900\n",
      "Epoch: 0650 cost= 0.189394370\n",
      "Epoch: 0700 cost= 0.180124119\n",
      "Epoch: 0750 cost= 0.171950743\n",
      "Epoch: 0800 cost= 0.164761946\n",
      "Epoch: 0850 cost= 0.158465892\n",
      "Epoch: 0900 cost= 0.152972698\n",
      "Epoch: 0950 cost= 0.148187578\n",
      "Epoch: 1000 cost= 0.144012585\n",
      "Optimization Finished!\n",
      "AUC: 0.981886165351\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # create log writer objects separately for training and testing\n",
    "    writer = tf.summary.FileWriter(logs_path + \"/train\",sess.graph)\n",
    "    writer2 = tf.summary.FileWriter(logs_path + \"/test\",sess.graph)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)           \n",
    "        _, c,summary,to = sess.run([optimizer, cost,common_op,training_ops], feed_dict={x: train_x, y: train_y})\n",
    "        # Compute average loss\n",
    "        avg_cost += c \n",
    "        writer.add_summary(summary,(epoch+1)*100)\n",
    "        writer.add_summary(to,(epoch+1)*100)\n",
    "        writer.flush()\n",
    "        tc,s2 = sess.run([cost,common_op], feed_dict={x: test_x, y: test_y})\n",
    "        writer2.add_summary (s2, (epoch+1) * 100)\n",
    "        writer2.flush()\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % 50 == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "    writer.flush()\n",
    "    \n",
    "    # we are adding additional histogram summaries here...\n",
    "    pred_summary = tf.summary.histogram(\"prediction\",pred)\n",
    "    merged = tf.summary.merge([pred_summary])\n",
    "    y_pred,ss = sess.run([pred,merged], feed_dict={x: test_x, y: test_y})\n",
    "    writer2.add_summary(ss)\n",
    "    writer2.flush()\n",
    "    print (\"AUC:\",sk.metrics.roc_auc_score(test_y, y_pred))\n",
    "    fpr, tpr, thresholds = sk.metrics.roc_curve(test_y, y_pred)\n",
    "    # we are adding ROC here\n",
    "    total = len(fpr)\n",
    "    for idx in range(total):\n",
    "        summt = tf.Summary()\n",
    "        summt.value.add(tag=\"ROC\", simple_value = tpr[idx])\n",
    "        writer2.add_summary (summt, fpr[idx] * 100) #act as global_step\n",
    "        writer2.flush ()\n",
    "        \n",
    "    writer.close()\n",
    "    writer2.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
